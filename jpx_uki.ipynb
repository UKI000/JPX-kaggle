{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e628e9a5",
   "metadata": {},
   "source": [
    "# はじめに（概要、留意事項、免責事項など）\n",
    "- Kaggleで開催されたJPX Tokyo Stock Exchange Prediction（ https://www.kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction ）の解法コードとなります。\n",
    "- コンペ提出時にエラーが発生したため、提出コードに対して一部コードを修正しています。\n",
    "- Kaggleで提供されたデータセットは本GitHubでは提供できません。Kaggleにてダウンロードをお願いします。\n",
    "- Kaggleで提供されたデータセットと、JPXが提供するAPI（JQuants API）で取得できるデータとは、一部整合性がありません。必要に応じて本コードに修正が必要です。\n",
    "- 本コードはコンペで提供されたオプションデータを使います。現在、JPXのオプションデータを取得する手段がないため、本コードはそのままでは将来的に使うことができません。必要に応じて修正が必要です。\n",
    "- 開発環境により、機械学習モデルの再現性がなくなる場合があります。本GitHubで提供する学習済みモデルは、GCPの16コアで学習しています。シードを指定した場合でもコア数等が異なる場合、モデルの再現性がなくなる場合があります。\n",
    "- 本コードに対する質問は、マケデコDiscord（リンク： https://t.co/gyU7O50Wb5 ）にお願いします。ただし全てに回答するとは限りませんのでご承知おき願います。\n",
    "- 本コードを用いたことで発生するいかなる損害にも応対はできません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e86ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import time, datetime\n",
    "import sys, gc, random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import lightgbm as lgb\n",
    "import umap.umap_ as umap\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "def pickle_dump(obj, path):\n",
    "    with open(path, mode=\"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def pickle_load(path):\n",
    "    with open(path, mode=\"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9279ba5",
   "metadata": {},
   "source": [
    "# JPX Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JPX(object):\n",
    "    def __init__(self, isTRAIN, USE_SUPPLEMENT):\n",
    "        self.isTRAIN        = isTRAIN\n",
    "        self.use_supplement = USE_SUPPLEMENT\n",
    "        self.stock_list     = None\n",
    "        self.univ_codes     = []\n",
    "        self.topix100_codes = []\n",
    "        self.stock_prices   = None\n",
    "        self.stock_fins     = None\n",
    "        self.markets        = None\n",
    "        self.options        = None\n",
    "        self.fundamentals   = []\n",
    "        self.data_dir          = \"input/jpx-tokyo-stock-exchange-prediction/\"\n",
    "        self.external_data_dir = \"input/jpx-external-data/\"\n",
    "        \n",
    "        self.tick           = None\n",
    "        self.limit          = None\n",
    "        \n",
    "    def _load_list(self):\n",
    "        tmp = pd.read_csv(f\"{self.data_dir}stock_list.csv\")\n",
    "        tmp = tmp[tmp[\"Universe0\"]==True]\n",
    "        tmp[\"33SectorCode\"] = tmp[\"33SectorCode\"].replace(\"-\", 0)\n",
    "        tmp[\"33SectorCode\"] = tmp[\"33SectorCode\"].astype(\"int\")\n",
    "        self.stock_list = tmp\n",
    "        self.univ_codes = [c for c in sorted(tmp[\"SecuritiesCode\"].unique())]\n",
    "        self.topix100_codes= [c for c in tmp.loc[tmp[\"NewIndexSeriesSizeCode\"].isin([\"1\", \"2\"]), \"SecuritiesCode\"]]\n",
    "        \n",
    "    def _load_fins(self):\n",
    "        tmp = pd.read_csv(f\"{self.data_dir}train_files/financials.csv\")\n",
    "        if self.use_supplement:\n",
    "            tmp2 = pd.read_csv(f\"{self.data_dir}supplemental_files/financials.csv\")\n",
    "            tmp = pd.concat([tmp, tmp2])\n",
    "            tmp = tmp.drop_duplicates(subset=[\"Date\", \"SecuritiesCode\"], keep=\"last\")\n",
    "            tmp = tmp.sort_values(\"Date\")\n",
    "        self.stock_fins = self._correct_fins(tmp)\n",
    "        \n",
    "    def _load_markets(self):\n",
    "        tmp = pd.read_csv(f\"{self.data_dir}train_files/secondary_stock_prices.csv\")\n",
    "        if self.use_supplement:\n",
    "            tmp2 = pd.read_csv(f\"{self.data_dir}supplemental_files/secondary_stock_prices.csv\")\n",
    "            tmp = pd.concat([tmp, tmp2])\n",
    "            tmp = tmp.drop_duplicates(subset=[\"Date\", \"SecuritiesCode\"], keep=\"last\")\n",
    "            tmp = tmp.sort_values(\"Date\")\n",
    "        if not self.isTRAIN:\n",
    "            tmp = tmp[tmp[\"Date\"]>=\"2019-12-01\"]\n",
    "        self.markets = self._secondary_to_market(tmp)\n",
    "        \n",
    "    def _load_prices(self):\n",
    "        tmp = pd.read_csv(f\"{self.data_dir}train_files/stock_prices.csv\")\n",
    "        if self.use_supplement:\n",
    "            tmp2 = pd.read_csv(f\"{self.data_dir}supplemental_files/stock_prices.csv\")\n",
    "            tmp = pd.concat([tmp, tmp2])\n",
    "            tmp = tmp.drop_duplicates(subset=[\"Date\", \"SecuritiesCode\"], keep=\"last\")\n",
    "            tmp = tmp.sort_values(\"Date\")\n",
    "        if not self.isTRAIN:\n",
    "            tmp = tmp[tmp[\"Date\"]>=\"2019-12-01\"]\n",
    "        self.stock_prices = self._correct_prices(tmp)\n",
    "    \n",
    "    def _load_options(self):\n",
    "        tmp = pd.read_csv(f\"{self.data_dir}train_files/options.csv\")\n",
    "        if self.use_supplement:\n",
    "            tmp2 = pd.read_csv(f\"{self.data_dir}supplemental_files/options.csv\")\n",
    "            tmp = pd.concat([tmp, tmp2])\n",
    "            tmp = tmp.drop_duplicates(subset=[\"Date\", \"OptionsCode\"], keep=\"last\")\n",
    "            tmp = tmp.sort_values(\"Date\")\n",
    "        if not self.isTRAIN:\n",
    "            tmp = tmp[tmp[\"Date\"]>=\"2019-12-01\"]\n",
    "        self.options = self._correct_options(tmp)\n",
    "    \n",
    "    def _load_tick(self):\n",
    "        self.tick = np.array(pd.read_csv(f\"{self.external_data_dir}tick.csv\"))\n",
    "    \n",
    "    def _load_limit(self):\n",
    "        self.limit = np.array(pd.read_csv(f\"{self.external_data_dir}limit.csv\"))\n",
    "        \n",
    "    def initialize(self):\n",
    "        self._load_list()\n",
    "        self._load_tick()\n",
    "        self._load_limit()\n",
    "        self._load_fins()\n",
    "        self._load_markets()\n",
    "        self._load_prices()\n",
    "        self._load_options()\n",
    "        \n",
    "    def _correct_fins(self, tmp_fins):\n",
    "        tmp_fins = tmp_fins.dropna(subset=[\"DisclosureNumber\"])\n",
    "        tmp_fins = tmp_fins.replace(\"－\", np.nan)\n",
    "        tmp_fins = tmp_fins[~tmp_fins[\"TypeOfDocument\"].str.contains(\"OtherPeriod\")]\n",
    "        \n",
    "        tmp_fins[\"Date\"] = pd.to_datetime(tmp_fins[\"Date\"])\n",
    "        \n",
    "        # simplify document type\n",
    "        tmp_fins[\"TypeOfDocument\"] = tmp_fins[\"TypeOfDocument\"].apply(lambda x: x.replace(\"FY\", \"4Q\"))\n",
    "        tmp_fins[\"TypeOfDocument\"] = tmp_fins[\"TypeOfDocument\"].apply(lambda x: x.replace(\"_Consolidated\", \"\"))\n",
    "        tmp_fins[\"TypeOfDocument\"] = tmp_fins[\"TypeOfDocument\"].apply(lambda x: x.replace(\"_NonConsolidated\", \"\"))\n",
    "        tmp_fins[\"TypeOfDocument\"] = tmp_fins[\"TypeOfDocument\"].apply(lambda x: x.replace(\"_JP\", \"\"))\n",
    "        tmp_fins[\"TypeOfDocument\"] = tmp_fins[\"TypeOfDocument\"].apply(lambda x: x.replace(\"_US\", \"\"))\n",
    "        tmp_fins[\"TypeOfDocument\"] = tmp_fins[\"TypeOfDocument\"].apply(lambda x: x.replace(\"_IFRS\", \"\"))\n",
    "        tmp_fins[\"TypeOfDocument\"] = tmp_fins[\"TypeOfDocument\"].apply(lambda x: x.replace(\"FinancialStatements\", \"_FS\"))\n",
    "        tmp_fins[\"TypeOfDocument\"] = tmp_fins[\"TypeOfDocument\"].apply(lambda x: x.replace(\"ForecastRevision\", \"Revision\"))\n",
    "        tmp_fins[\"TypeOfDocument\"] = tmp_fins[\"TypeOfDocument\"].apply(lambda x: x.replace(\"NumericalCorrection\", \"Revision\"))\n",
    "        \n",
    "        # make sequential period\n",
    "        tmp_fins[\"FiscalYear\"] = tmp_fins[\"CurrentFiscalYearStartDate\"].apply(lambda x: float(x[:4]))\n",
    "        tmp_fins[\"Period\"] = tmp_fins[\"TypeOfDocument\"].map({\"1Q_FS\":0, \"2Q_FS\":0.25, \"3Q_FS\":0.5, \"4Q_FS\":0.75, \"Revision\":np.nan}) + tmp_fins[\"FiscalYear\"]\n",
    "        tmp_fins[\"Period\"] = tmp_fins.groupby(\"SecuritiesCode\").ffill()[\"Period\"]\n",
    "        tmp_fins = tmp_fins.dropna(subset=[\"Period\"])\n",
    "        \n",
    "        # pick up columns\n",
    "        tmp_fins = tmp_fins[[\"Date\", \"SecuritiesCode\", \"TypeOfDocument\", \"NetSales\", \"OperatingProfit\", \"OrdinaryProfit\", \"Profit\", \n",
    "                         \"EarningsPerShare\", \"TotalAssets\", \"Equity\", \"EquityToAssetRatio\", \"BookValuePerShare\",\n",
    "                         \"ForecastNetSales\", \"ForecastOperatingProfit\", \"ForecastOrdinaryProfit\", \"ForecastProfit\",\n",
    "                         \"ForecastEarningsPerShare\", \"AverageNumberOfShares\", \"FiscalYear\", \"Period\"]]\n",
    "        \n",
    "        # rename\n",
    "        tmp_fins.rename(columns={\"EarningsPerShare\":\"EPS\", \"EquityToAssetRatio\":\"EquityRatio\", \"BookValuePerShare\":\"BPS\",\n",
    "                             \"ForecastEarningsPerShare\":\"ForecastEPS\", \"AverageNumberOfShares\":\"Shares\"}, inplace=True)\n",
    "        \n",
    "        # data types\n",
    "        float_n = [\"EPS\", \"EquityRatio\", \"BPS\", \"ForecastEPS\"]\n",
    "        float_b = [\"NetSales\", \"OperatingProfit\", \"OrdinaryProfit\", \"Profit\", \"TotalAssets\", \"Equity\",\n",
    "               \"ForecastNetSales\", \"ForecastOperatingProfit\", \"ForecastOrdinaryProfit\", \"ForecastProfit\", \"Shares\"]\n",
    "        \n",
    "        for f in float_n:\n",
    "            tmp_fins[f] = tmp_fins[f].astype(\"float\")\n",
    "        \n",
    "        for f in float_b:\n",
    "            tmp_fins[f] = tmp_fins[f].astype(\"float\")/1000000000\n",
    "        \n",
    "        # pick up stock in the universe\n",
    "        tmp_fins = tmp_fins[tmp_fins[\"SecuritiesCode\"].isin(self.univ_codes)]\n",
    "        \n",
    "        return tmp_fins\n",
    "        \n",
    "    def _secondary_to_market(self, tmp_secondary):\n",
    "        tmp_secondary[\"Date\"] = pd.to_datetime(tmp_secondary[\"Date\"])\n",
    "        tmp_markets = tmp_secondary.pivot(index=\"Date\", columns=\"SecuritiesCode\", values=\"Close\")\n",
    "        \n",
    "        topix_etf = [1305, 1306, 1308, 1348, 1473, 1475, 2524, 2557]\n",
    "        topix_markets = tmp_markets[topix_etf].copy()\n",
    "        topix_markets[\"TOPIX\"] = topix_markets.mean(axis=1)\n",
    "        topix_markets.reset_index(inplace=True)\n",
    "        topix_markets = topix_markets[[\"Date\", \"TOPIX\"]]\n",
    "        \n",
    "        nikkei_etf = [1320, 1321, 1329, 1330, 1346, 1369, 1397, 2525]\n",
    "        nikkei_markets = tmp_markets[nikkei_etf].copy()\n",
    "        nikkei_markets[\"NIKKEI\"] = nikkei_markets.mean(axis=1)\n",
    "        nikkei_markets.reset_index(inplace=True)\n",
    "        nikkei_markets = nikkei_markets[[\"Date\", \"NIKKEI\"]]\n",
    "        \n",
    "        tmp_markets = pd.merge(topix_markets, nikkei_markets, on=\"Date\")\n",
    "        \n",
    "        return tmp_markets\n",
    "        \n",
    "    def _correct_prices(self, tmp_prices):\n",
    "        tmp_prices[\"Date\"] = pd.to_datetime(tmp_prices[\"Date\"])\n",
    "        \n",
    "        def get_limit(price):\n",
    "            try:\n",
    "                return self.limit[self.limit[:, 0]>price, 1][0]\n",
    "            except:\n",
    "                return np.nan\n",
    "        \n",
    "        def get_large_stock_tick(price):\n",
    "            try:\n",
    "                return self.tick[self.tick[:, 0]>price, 1][0]\n",
    "            except:\n",
    "                return np.nan\n",
    "        \n",
    "        def get_small_stock_tick(price):\n",
    "            try:\n",
    "                return self.tick[self.tick[:, 0]>price, 2][0]\n",
    "            except:\n",
    "                return np.nan\n",
    "        \n",
    "        tmp_prices[\"limit\"] = tmp_prices[\"Close\"].apply(get_limit)\n",
    "        tmp_prices[\"large_stock_tick\"] = tmp_prices[\"Close\"].apply(get_large_stock_tick)\n",
    "        tmp_prices[\"small_stock_tick\"] = tmp_prices[\"Close\"].apply(get_small_stock_tick)\n",
    "        \n",
    "        return tmp_prices\n",
    "    \n",
    "    def _correct_options(self, tmp_options):\n",
    "        tmp_options[\"Date\"] = pd.to_datetime(tmp_options[\"Date\"])\n",
    "        \n",
    "        tmp_options = tmp_options[tmp_options[\"StrikePrice\"]%500==0]\n",
    "        tmp_options[\"month\"] = tmp_options[\"ContractMonth\"]%100\n",
    "        tmp_options = tmp_options[tmp_options[\"month\"].isin([3, 6, 9, 12])]\n",
    "        \n",
    "        return tmp_options\n",
    "    \n",
    "    def _get_fundas(self, code):\n",
    "        tmp = self.stock_fins[self.stock_fins[\"SecuritiesCode\"]==code].copy()\n",
    "        \n",
    "        periods = [p for p in sorted(tmp[\"Period\"].unique())]\n",
    "        cols = [\"NetSales\", \"OperatingProfit\", \"OrdinaryProfit\", \"Profit\", \"EPS\", \"TotalAssets\", \"Equity\", \"EquityRatio\", \"BPS\",\n",
    "                \"ForecastNetSales\", \"ForecastOperatingProfit\", \"ForecastOrdinaryProfit\", \"ForecastProfit\", \"ForecastEPS\", \"Shares\"]\n",
    "        \n",
    "        # fs is the latest and modified financial statement at each date in \"financials\"\n",
    "        fs = pd.DataFrame(index=periods, columns=[\"comp\"])\n",
    "        fs.index.name=\"Period\"\n",
    "        fs[cols] = np.nan\n",
    "        \n",
    "        res = []\n",
    "        \n",
    "        for i in range(len(tmp)):\n",
    "            # update fs\n",
    "            if tmp[\"TypeOfDocument\"].iloc[i] == \"Revision\":\n",
    "                tmp_ser = tmp.iloc[i]\n",
    "                tmp_cols = [f for f in tmp_ser[~tmp_ser.isna()].index if f not in [\"Date\", \"SecuritiesCode\", \"TypeOfDocument\", \"FiscalYear\", \"Period\"]]\n",
    "                if len(tmp_cols) != 0:\n",
    "                    fs.loc[tmp[\"Period\"].iloc[i], tmp_cols] = tmp_ser[tmp_cols]\n",
    "            else:\n",
    "                fs.loc[tmp[\"Period\"].iloc[i], \"comp\"] = 1\n",
    "                fs.loc[tmp[\"Period\"].iloc[i], cols] = tmp[cols].iloc[i]\n",
    "            \n",
    "            # make feature based on fs\n",
    "            tmp_fs = fs.dropna(subset=[\"comp\"])\n",
    "            tmp_fs = tmp_fs.ffill()\n",
    "            tmp_fs.reset_index(inplace=True)\n",
    "            tmp_fs[\"PeriodDiff\"] = tmp_fs[\"Period\"].diff()\n",
    "            tmp_fs[\"Period\"] = tmp_fs[\"Period\"]%1\n",
    "            \n",
    "            # quarterly adjustment of Profit Loss\n",
    "            tmp_cols = [\"NetSales\", \"OperatingProfit\", \"OrdinaryProfit\", \"Profit\"]\n",
    "            for f in tmp_cols:\n",
    "                tmp_fs[\"Yearly\"+f] = tmp_fs[f]\n",
    "                tmp_fs.loc[tmp_fs[\"Period\"]!=0, \"Yearly\"+f] = tmp_fs[f].diff() * (0.25/tmp_fs[\"PeriodDiff\"])\n",
    "                tmp_fs[\"Yearly\"+f] = tmp_fs[\"Yearly\"+f] * 4\n",
    "            \n",
    "            # EPS\n",
    "            tmp_fs[\"YearlyEPS\"] = tmp_fs[\"YearlyProfit\"] / tmp_fs[\"Shares\"]\n",
    "            \n",
    "            # raw feature\n",
    "            tmp_fs[\"Expense1\"] = tmp_fs[\"YearlyNetSales\"] - tmp_fs[\"YearlyOperatingProfit\"]\n",
    "            tmp_fs[\"Expense2\"] = tmp_fs[\"YearlyOperatingProfit\"] - tmp_fs[\"YearlyOrdinaryProfit\"]\n",
    "            tmp_fs[\"Expense3\"] = tmp_fs[\"YearlyOrdinaryProfit\"] - tmp_fs[\"YearlyProfit\"]\n",
    "            \n",
    "            tmp_fs[\"ForecastExpense1\"] = tmp_fs[\"ForecastNetSales\"] - tmp_fs[\"ForecastOperatingProfit\"]\n",
    "            tmp_fs[\"ForecastExpense2\"] = tmp_fs[\"ForecastOperatingProfit\"] - tmp_fs[\"ForecastOrdinaryProfit\"]\n",
    "            tmp_fs[\"ForecastExpense3\"] = tmp_fs[\"ForecastOrdinaryProfit\"] - tmp_fs[\"ForecastProfit\"]\n",
    "            \n",
    "            # ratio feature\n",
    "            tmp_fs[\"ProfitMargin1\"] = tmp_fs[\"Profit\"] / tmp_fs[\"NetSales\"]\n",
    "            tmp_fs[\"ProfitMargin2\"] = tmp_fs[\"OrdinaryProfit\"] / tmp_fs[\"NetSales\"]\n",
    "            tmp_fs[\"ProfitMargin3\"] = tmp_fs[\"OperatingProfit\"] / tmp_fs[\"NetSales\"]\n",
    "            \n",
    "            tmp_fs[\"ForecastProfitMargin1\"] = tmp_fs[\"ForecastProfit\"] / tmp_fs[\"ForecastNetSales\"]\n",
    "            tmp_fs[\"ForecastProfitMargin2\"] = tmp_fs[\"ForecastOrdinaryProfit\"] / tmp_fs[\"ForecastNetSales\"]\n",
    "            tmp_fs[\"ForecastProfitMargin3\"] = tmp_fs[\"ForecastOperatingProfit\"] / tmp_fs[\"ForecastNetSales\"]\n",
    "            \n",
    "            tmp_fs[\"ROE1\"] = tmp_fs[\"Profit\"] / tmp_fs[\"Equity\"]\n",
    "            tmp_fs[\"ROE2\"] = tmp_fs[\"OrdinaryProfit\"] / tmp_fs[\"Equity\"]\n",
    "            tmp_fs[\"ROE3\"] = tmp_fs[\"OperatingProfit\"] / tmp_fs[\"Equity\"]\n",
    "            \n",
    "            tmp_fs[\"ForecastROE1\"] = tmp_fs[\"ForecastProfit\"] / tmp_fs[\"Equity\"]\n",
    "            tmp_fs[\"ForecastROE2\"] = tmp_fs[\"ForecastOrdinaryProfit\"] / tmp_fs[\"Equity\"]\n",
    "            tmp_fs[\"ForecastROE3\"] = tmp_fs[\"ForecastOperatingProfit\"] / tmp_fs[\"Equity\"]\n",
    "            \n",
    "            tmp_fs[\"ROA1\"] = tmp_fs[\"Profit\"] / tmp_fs[\"TotalAssets\"]\n",
    "            tmp_fs[\"ROA2\"] = tmp_fs[\"OrdinaryProfit\"] / tmp_fs[\"TotalAssets\"]\n",
    "            tmp_fs[\"ROA3\"] = tmp_fs[\"OperatingProfit\"] / tmp_fs[\"TotalAssets\"]\n",
    "            \n",
    "            tmp_fs[\"ForecastROA1\"] = tmp_fs[\"ForecastProfit\"] / tmp_fs[\"TotalAssets\"]\n",
    "            tmp_fs[\"ForecastROA2\"] = tmp_fs[\"ForecastOrdinaryProfit\"] / tmp_fs[\"TotalAssets\"]\n",
    "            tmp_fs[\"ForecastROA3\"] = tmp_fs[\"ForecastOperatingProfit\"] / tmp_fs[\"TotalAssets\"]\n",
    "            \n",
    "            tmp_fs[\"CostRatio1\"] = (tmp_fs[\"NetSales\"] - tmp_fs[\"OperatingProfit\"]) / tmp_fs[\"NetSales\"]\n",
    "            tmp_fs[\"CostRatio2\"] = (tmp_fs[\"OperatingProfit\"] - tmp_fs[\"OrdinaryProfit\"]) / tmp_fs[\"NetSales\"]\n",
    "            tmp_fs[\"CostRatio3\"] = (tmp_fs[\"OrdinaryProfit\"] - tmp_fs[\"Profit\"]) / tmp_fs[\"NetSales\"]\n",
    "            \n",
    "            tmp_fs[\"ForecastCostRatio1\"] = tmp_fs[\"ForecastExpense1\"] / tmp_fs[\"ForecastNetSales\"]\n",
    "            tmp_fs[\"ForecastCostRatio2\"] = tmp_fs[\"ForecastExpense2\"] / tmp_fs[\"ForecastNetSales\"]\n",
    "            tmp_fs[\"ForecastCostRatio3\"] = tmp_fs[\"ForecastExpense3\"] / tmp_fs[\"ForecastNetSales\"]\n",
    "            \n",
    "            tmp_fs[\"TurnOver\"] = tmp_fs[\"NetSales\"] / tmp_fs[\"TotalAssets\"]\n",
    "            tmp_fs[\"ForecastTurnOver\"] = tmp_fs[\"ForecastNetSales\"] / tmp_fs[\"TotalAssets\"]\n",
    "            \n",
    "            tmp_fs = tmp_fs.replace([-np.inf, np.inf], np.nan)\n",
    "            \n",
    "            # feature lists\n",
    "            feats1 = [\"Period\"]\n",
    "            \n",
    "            feats2 = [\"YearlyNetSales\", \"YearlyOperatingProfit\", \"YearlyOrdinaryProfit\", \"YearlyProfit\",\n",
    "                      \"ForecastNetSales\", \"ForecastOperatingProfit\", \"ForecastOrdinaryProfit\", \"ForecastProfit\",\n",
    "                      \"Expense1\", \"Expense2\", \"Expense3\", \"ForecastExpense1\", \"ForecastExpense2\", \"ForecastExpense3\",\n",
    "                      \"TotalAssets\", \"Equity\"]\n",
    "            \n",
    "            feats3 = [\"ProfitMargin1\", \"ProfitMargin2\", \"ProfitMargin3\",\n",
    "                      \"ForecastProfitMargin1\", \"ForecastProfitMargin2\", \"ForecastProfitMargin3\",\n",
    "                      \"ROE1\", \"ROE2\", \"ROE3\", \"ForecastROE1\", \"ForecastROE2\", \"ForecastROE3\",\n",
    "                      \"ROA1\", \"ROA2\", \"ROA3\", \"ForecastROA1\", \"ForecastROA2\", \"ForecastROA3\",\n",
    "                      \"CostRatio1\", \"CostRatio2\", \"CostRatio3\", \"ForecastCostRatio1\", \"ForecastCostRatio2\", \"ForecastCostRatio3\",\n",
    "                      \"TurnOver\", \"ForecastTurnOver\", \"EquityRatio\"]\n",
    "            \n",
    "            feats4 = [\"BPS\", \"YearlyEPS\", \"ForecastEPS\"]\n",
    "            \n",
    "            # diff feature: diff means diff(1), dev means Deviation between Results and Past Forecasts\n",
    "            d_feats1 = [\"DevNetSales\", \"DevOperatingProfit\", \"DevOrdinaryProfit\", \"DevProfit\",\n",
    "                        \"DevExpense1\", \"DevExpense2\", \"DevExpense3\", \"DevProfitMargin1\", \"DevProfitMargin2\", \"DevProfitMargin3\",\n",
    "                        \"DevROE1\", \"DevROE2\", \"DevROE3\", \"DevROA1\", \"DevROA2\", \"DevROA3\",\n",
    "                        \"DevCostRatio1\", \"DevCostRatio2\", \"DevCostRatio3\"]\n",
    "            \n",
    "            d_feats2 = []\n",
    "            for f in feats2:\n",
    "                d_feats2.append(\"Diff\"+f)\n",
    "                tmp_fs[\"Diff\"+f] = tmp_fs[f].diff(1)\n",
    "            \n",
    "            d_feats3 = []\n",
    "            for f in feats3:\n",
    "                d_feats3.append(\"Diff\"+f)\n",
    "                tmp_fs[\"Diff\"+f] = tmp_fs[f].diff(1)\n",
    "            \n",
    "            tmp_fs[\"DevNetSales\"] = tmp_fs[\"YearlyNetSales\"] - tmp_fs[\"ForecastNetSales\"].shift(1)\n",
    "            tmp_fs[\"DevOperatingProfit\"] = tmp_fs[\"YearlyOperatingProfit\"] - tmp_fs[\"ForecastOperatingProfit\"].shift(1)\n",
    "            tmp_fs[\"DevOrdinaryProfit\"] = tmp_fs[\"YearlyOrdinaryProfit\"] - tmp_fs[\"ForecastOrdinaryProfit\"].shift(1)\n",
    "            tmp_fs[\"DevProfit\"] = tmp_fs[\"YearlyProfit\"] - tmp_fs[\"ForecastProfit\"].shift(1)\n",
    "            \n",
    "            tmp_fs[\"DevExpense1\"] = tmp_fs[\"Expense1\"] - tmp_fs[\"ForecastExpense1\"].shift(1)\n",
    "            tmp_fs[\"DevExpense2\"] = tmp_fs[\"Expense2\"] - tmp_fs[\"ForecastExpense2\"].shift(1)\n",
    "            tmp_fs[\"DevExpense3\"] = tmp_fs[\"Expense3\"] - tmp_fs[\"ForecastExpense3\"].shift(1)\n",
    "            \n",
    "            tmp_fs[\"DevProfitMargin1\"] = tmp_fs[\"ProfitMargin1\"] - tmp_fs[\"ForecastProfitMargin1\"].shift(1)\n",
    "            tmp_fs[\"DevProfitMargin2\"] = tmp_fs[\"ProfitMargin2\"] - tmp_fs[\"ForecastProfitMargin2\"].shift(1)\n",
    "            tmp_fs[\"DevProfitMargin3\"] = tmp_fs[\"ProfitMargin3\"] - tmp_fs[\"ForecastProfitMargin3\"].shift(1)\n",
    "            \n",
    "            tmp_fs[\"DevROE1\"] = tmp_fs[\"ROE1\"] - tmp_fs[\"ForecastROE1\"].shift(1)\n",
    "            tmp_fs[\"DevROE2\"] = tmp_fs[\"ROE2\"] - tmp_fs[\"ForecastROE2\"].shift(1)\n",
    "            tmp_fs[\"DevROE3\"] = tmp_fs[\"ROE3\"] - tmp_fs[\"ForecastROE3\"].shift(1)\n",
    "            \n",
    "            tmp_fs[\"DevROA1\"] = tmp_fs[\"ROA1\"] - tmp_fs[\"ForecastROA1\"].shift(1)\n",
    "            tmp_fs[\"DevROA2\"] = tmp_fs[\"ROA2\"] - tmp_fs[\"ForecastROA2\"].shift(1)\n",
    "            tmp_fs[\"DevROA3\"] = tmp_fs[\"ROA3\"] - tmp_fs[\"ForecastROA3\"].shift(1)\n",
    "            \n",
    "            tmp_fs[\"DevCostRatio1\"] = tmp_fs[\"CostRatio1\"] - tmp_fs[\"ForecastCostRatio1\"].shift(1)\n",
    "            tmp_fs[\"DevCostRatio2\"] = tmp_fs[\"CostRatio2\"] - tmp_fs[\"ForecastCostRatio2\"].shift(1)\n",
    "            tmp_fs[\"DevCostRatio3\"] = tmp_fs[\"CostRatio3\"] - tmp_fs[\"ForecastCostRatio3\"].shift(1)\n",
    "            \n",
    "            # pick up columns\n",
    "            tmp_fs[\"Date\"] = tmp[\"Date\"].iloc[i]\n",
    "            tmp_fs[\"SecuritiesCode\"] = tmp[\"SecuritiesCode\"].iloc[i]\n",
    "            \n",
    "            feats = [\"Date\", \"SecuritiesCode\"]\n",
    "            feats.extend(feats1)\n",
    "            feats.extend(feats2)\n",
    "            feats.extend(feats3)\n",
    "            feats.extend(feats4)\n",
    "            feats.extend(d_feats1)\n",
    "            feats.extend(d_feats2)\n",
    "            feats.extend(d_feats3)\n",
    "            \n",
    "            res.append(tmp_fs[feats].iloc[-1])\n",
    "        \n",
    "        res = pd.DataFrame(res)\n",
    "        res = res.drop_duplicates(keep=\"last\", subset=[\"Date\"])\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def _get_techs(self, code):\n",
    "        tmp = self.stock_prices[self.stock_prices[\"SecuritiesCode\"]==code].copy()\n",
    "        \n",
    "        # adjust price data\n",
    "        adj = tmp[tmp[\"AdjustmentFactor\"]!=1]\n",
    "        if len(adj) >= 2:\n",
    "            for i in reversed(range(len(adj))):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                adj[\"AdjustmentFactor\"].iloc[i-1] *= adj[\"AdjustmentFactor\"].iloc[i]\n",
    "        \n",
    "        tmp.loc[tmp[\"RowId\"].isin(adj[\"RowId\"]), \"AdjustmentFactor\"] = adj[\"AdjustmentFactor\"]\n",
    "        tmp.loc[tmp[\"AdjustmentFactor\"]==1, \"AdjustmentFactor\"] = np.nan\n",
    "        tmp[\"AdjustmentFactor\"] = tmp[\"AdjustmentFactor\"].bfill()\n",
    "        \n",
    "        tmp.loc[np.isnan(tmp[\"AdjustmentFactor\"]), \"AdjustmentFactor\"] = 1\n",
    "        \n",
    "        for f in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]:\n",
    "            if f == \"Volume\":\n",
    "                tmp[f] = tmp[f] / tmp[\"AdjustmentFactor\"]\n",
    "            else:\n",
    "                tmp[f] = tmp[f] * tmp[\"AdjustmentFactor\"]\n",
    "        \n",
    "        # merge market data\n",
    "        tmp = tmp.merge(self.markets, on=\"Date\", how=\"left\")\n",
    "        \n",
    "        # technical feature\n",
    "        tmp[\"ror1\"] = tmp[\"Close\"].pct_change(1)\n",
    "        tmp[\"ror22\"] = tmp[\"Close\"].pct_change(21).shift(1)\n",
    "        tmp[\"ror252\"] = tmp[\"Close\"].pct_change(230).shift(22)\n",
    "        tmp[\"topix_ror1\"] = tmp[\"TOPIX\"].pct_change(1)\n",
    "        tmp[\"nikkei_ror1\"] = tmp[\"NIKKEI\"].pct_change(1)\n",
    "        \n",
    "        tmp[\"volatility252\"] = tmp[\"ror1\"].rolling(252).std()\n",
    "        tmp[\"topix_corr\"] = tmp[\"ror1\"].rolling(252).corr(tmp[\"topix_ror1\"])\n",
    "        tmp[\"nikkei_corr\"] = tmp[\"ror1\"].rolling(252).corr(tmp[\"nikkei_ror1\"])\n",
    "        \n",
    "        tmp[\"Volume\"] = tmp[\"Volume\"].fillna(0)\n",
    "        tmp[\"Volume5\"] = tmp[\"Volume\"].rolling(5).mean()\n",
    "        tmp[\"Volume20\"] = tmp[\"Volume\"].rolling(20).mean()\n",
    "        tmp[\"Volume60\"] = tmp[\"Volume\"].rolling(60).mean()\n",
    "        \n",
    "        tmp[\"trading_value\"] = tmp[\"Volume\"]*tmp[\"Close\"]\n",
    "        tmp[\"trading_value5\"] = tmp[\"trading_value\"].rolling(5).mean()\n",
    "        tmp[\"trading_value20\"] = tmp[\"trading_value\"].rolling(20).mean()\n",
    "        tmp[\"trading_value60\"] = tmp[\"trading_value\"].rolling(60).mean()\n",
    "        \n",
    "        tmp[\"range_rt\"] = np.abs(tmp[\"ror1\"])\n",
    "        tmp[\"prev_close\"] = tmp[\"Close\"].shift(1)\n",
    "        tmp[\"range_hl\"] = np.abs( (tmp[[\"High\", \"prev_close\"]].max(axis=1) - tmp[[\"Low\", \"prev_close\"]].min(axis=1)) / tmp[\"prev_close\"])\n",
    "        \n",
    "        tmp[\"marketImpact_rt\"] = (tmp[\"range_rt\"] / tmp[\"trading_value\"]).replace([-np.inf, np.inf], np.nan)\n",
    "        tmp[\"marketImpact_rt5\"] = tmp[\"marketImpact_rt\"].rolling(5).mean()\n",
    "        tmp[\"marketImpact_rt20\"] = tmp[\"marketImpact_rt\"].rolling(20).mean()\n",
    "        tmp[\"marketImpact_rt60\"] = tmp[\"marketImpact_rt\"].rolling(60).mean()\n",
    "        \n",
    "        tmp[\"marketImpact_hl\"] = (tmp[\"range_hl\"] / tmp[\"trading_value\"]).replace([-np.inf, np.inf], np.nan)\n",
    "        tmp[\"marketImpact_hl5\"] = tmp[\"marketImpact_hl\"].rolling(5).mean()\n",
    "        tmp[\"marketImpact_hl20\"] = tmp[\"marketImpact_hl\"].rolling(20).mean()\n",
    "        tmp[\"marketImpact_hl60\"] = tmp[\"marketImpact_hl\"].rolling(60).mean()\n",
    "        \n",
    "        tmp = tmp[[\"RowId\", \"Date\", \"SecuritiesCode\", \"Open\", \"High\", \"Low\", \"Close\", \"AdjustmentFactor\", \"Target\", \"limit\", \"large_stock_tick\", \"small_stock_tick\",\n",
    "                   \"prev_close\", \"ror1\", \"ror22\", \"ror252\", \"volatility252\", \"topix_corr\", \"nikkei_corr\",\n",
    "                   \"Volume\", \"Volume5\", \"Volume20\", \"Volume60\",\n",
    "                   \"trading_value\", \"trading_value5\", \"trading_value20\", \"trading_value60\",\n",
    "                   \"marketImpact_rt\", \"marketImpact_rt5\", \"marketImpact_rt20\", \"marketImpact_rt60\",\n",
    "                   \"marketImpact_hl\", \"marketImpact_hl5\", \"marketImpact_hl20\", \"marketImpact_hl60\"]]\n",
    "        \n",
    "        # merge fundamentals\n",
    "        tmp_fins = self.fundamentals[self.fundamentals[\"SecuritiesCode\"]==code]\n",
    "        tmp = tmp.merge(tmp_fins, on=[\"Date\", \"SecuritiesCode\"], how=\"left\")\n",
    "        tmp = tmp.ffill()\n",
    "        \n",
    "        # fundamentals feature\n",
    "        shares = self.stock_list.loc[self.stock_list[\"SecuritiesCode\"]==code, \"IssuedShares\"].values[0]\n",
    "        \n",
    "        tmp[\"market_cap\"] = tmp[\"Close\"] * (shares/1000000)\n",
    "        \n",
    "        tmp[\"CalcBPS\"] = tmp[\"Equity\"] / (shares/1000000000)\n",
    "        tmp[\"CalcYearlyEPS\"] = tmp[\"YearlyProfit\"] / (shares/1000000000)\n",
    "        tmp[\"CalcForecastEPS\"] = tmp[\"ForecastProfit\"] / (shares/1000000000)\n",
    "        \n",
    "        tmp[\"PBR\"] = tmp[\"Close\"] / tmp[\"CalcBPS\"]\n",
    "        tmp[\"PER\"] = tmp[\"Close\"] / tmp[\"CalcYearlyEPS\"]\n",
    "        tmp[\"ForecastPER\"] = tmp[\"Close\"] / tmp[\"CalcForecastEPS\"]\n",
    "        \n",
    "        tmp[\"volumeTurnover\"] = tmp[\"Volume\"] / (shares/1000)\n",
    "        tmp[\"volumeTurnover5\"] = tmp[\"volumeTurnover\"].rolling(5).mean()\n",
    "        tmp[\"volumeTurnover20\"] = tmp[\"volumeTurnover\"].rolling(20).mean()\n",
    "        tmp[\"volumeTurnover60\"] = tmp[\"volumeTurnover\"].rolling(60).mean()\n",
    "        \n",
    "        # ticks and limitations\n",
    "        tmp[\"sdaka\"]  = tmp[\"prev_close\"] + tmp[\"limit\"].shift(1)\n",
    "        tmp[\"syasu\"]  = tmp[\"prev_close\"] - tmp[\"limit\"].shift(1)\n",
    "        \n",
    "        tmp[\"stop_close\"] = 1\n",
    "        tmp.loc[(tmp[\"Close\"]==tmp[\"sdaka\"]) & (tmp[\"Open\"]!=tmp[\"sdaka\"]), \"stop_close\"] = 2\n",
    "        tmp.loc[(tmp[\"Close\"]==tmp[\"syasu\"]) & (tmp[\"Open\"]!=tmp[\"syasu\"]), \"stop_close\"] = 0\n",
    "        \n",
    "        tmp[\"stop_open\"] = 1\n",
    "        tmp.loc[(tmp[\"Close\"]==tmp[\"sdaka\"]) & (tmp[\"Open\"]==tmp[\"sdaka\"]), \"stop_open\"] = 2\n",
    "        tmp.loc[(tmp[\"Close\"]==tmp[\"syasu\"]) & (tmp[\"Open\"]==tmp[\"syasu\"]), \"stop_open\"] = 0\n",
    "        \n",
    "        tmp[\"limit_rate\"] = tmp[\"limit\"] / tmp[\"Close\"]\n",
    "        \n",
    "        tmp[\"tick\"] = np.nan\n",
    "        tmp.loc[tmp[\"SecuritiesCode\"].isin(self.topix100_codes), \"tick\"] = tmp[\"large_stock_tick\"]\n",
    "        tmp.loc[~tmp[\"SecuritiesCode\"].isin(self.topix100_codes), \"tick\"] = tmp[\"small_stock_tick\"]\n",
    "        \n",
    "        tmp[\"tick_rate\"] = tmp[\"tick\"] / tmp[\"Close\"]\n",
    "        \n",
    "        # uptick rule\n",
    "        tmp[\"uptick\"] = 0\n",
    "        tmp[\"down_rate\"] = tmp[\"Low\"] / tmp[\"prev_close\"]\n",
    "        tmp.loc[tmp[\"down_rate\"] <= 0.9, \"uptick\"] = 1\n",
    "        \n",
    "        # pick up feature\n",
    "        feats = [\"RowId\", \"Date\", \"SecuritiesCode\", \"Target\",\n",
    "                 \"Close\", \"ror1\", \"ror22\", \"ror252\", \"volatility252\", \"topix_corr\", \"nikkei_corr\",\n",
    "                 \"Volume\", \"Volume5\", \"Volume20\", \"Volume60\",\n",
    "                 \"trading_value\", \"trading_value5\", \"trading_value20\", \"trading_value60\",\n",
    "                 \"marketImpact_rt\", \"marketImpact_rt5\", \"marketImpact_rt20\", \"marketImpact_rt60\",\n",
    "                 \"marketImpact_hl\", \"marketImpact_hl5\", \"marketImpact_hl20\", \"marketImpact_hl60\",\n",
    "                 'market_cap', 'PBR', 'PER', 'ForecastPER',\n",
    "                 \"volumeTurnover\", \"volumeTurnover5\", \"volumeTurnover20\", \"volumeTurnover60\",\n",
    "                 \"tick_rate\", \"limit_rate\", \"stop_close\", \"stop_open\", \"uptick\"]\n",
    "        remove_feats = [\"Date\", \"SecuritiesCode\", \"Period\", \"BPS\", \"YearlyEPS\", \"ForecastEPS\"]\n",
    "        fins_feats   = [f for f in self.fundamentals.columns if f not in remove_feats]\n",
    "        feats.extend(fins_feats)\n",
    "        \n",
    "        tmp = tmp[feats]\n",
    "        \n",
    "        return tmp\n",
    "\n",
    "    def create_fundamentals(self):\n",
    "        #tmp = Parallel(n_jobs=-1, verbose=1)(delayed(self._get_fundas)(code) for code in self.univ_codes)\n",
    "        tmp = []\n",
    "        for code in tqdm(self.univ_codes):\n",
    "            tmp.append(self._get_fundas(code))\n",
    "            \n",
    "        self.fundamentals = pd.concat(tmp)\n",
    "        \n",
    "    def update_fins(self, iter_data):\n",
    "        new_data = self._correct_fins(iter_data)\n",
    "        if len(new_data) != 0:\n",
    "            self.stock_fins = pd.concat([self.stock_fins, new_data])\n",
    "            self.stock_fins = self.stock_fins.drop_duplicates(subset=[\"Date\", \"SecuritiesCode\"], keep=\"last\")\n",
    "            self.stock_fins = self.stock_fins.sort_values(\"Date\")\n",
    "    \n",
    "    def update_markets(self, iter_data):\n",
    "        new_data = self._secondary_to_market(iter_data)\n",
    "        self.markets = pd.concat([self.markets, new_data])\n",
    "        self.markets = self.markets.drop_duplicates(subset=[\"Date\"], keep=\"last\")\n",
    "        self.markets = self.markets.sort_values(\"Date\")\n",
    "    \n",
    "    def update_prices(self, iter_data):\n",
    "        new_data = self._correct_prices(iter_data)\n",
    "        self.stock_prices = pd.concat([self.stock_prices, new_data])\n",
    "        self.stock_prices = self.stock_prices.drop_duplicates(subset=[\"Date\", \"SecuritiesCode\"], keep=\"last\")\n",
    "        self.stock_prices = self.stock_prices.sort_values(\"Date\")\n",
    "    \n",
    "    def update_options(self, iter_data):\n",
    "        new_data = self._correct_options(iter_data)\n",
    "        self.options = pd.concat([self.options, new_data])\n",
    "        self.options = self.options.drop_duplicates(subset=[\"Date\", \"OptionsCode\"], keep=\"last\")\n",
    "        self.options = self.options.sort_values(\"Date\")\n",
    "        \n",
    "    def update_fundamentals(self, codes):\n",
    "        try:\n",
    "            new_fundamentals = []\n",
    "            for code in codes:\n",
    "                new_fundamentals.append(self._get_fundas(code))\n",
    "            \n",
    "            new_fundamentals = pd.concat(new_fundamentals)\n",
    "            \n",
    "            self.fundamentals = self.fundamentals[~self.fundamentals[\"SecuritiesCode\"].isin(codes)]   # remove updated stocks\n",
    "            self.fundamentals = pd.concat([self.fundamentals, new_fundamentals])                      # add updated stocks\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def get_feature(self):\n",
    "        df_feats = []\n",
    "        \n",
    "        for code in self.univ_codes:\n",
    "            df_feats.append(self._get_techs(code))\n",
    "        \n",
    "        df_feats = pd.concat(df_feats)\n",
    "        \n",
    "        # sectors\n",
    "        df_feats = df_feats.merge(self.stock_list[[\"SecuritiesCode\", \"33SectorCode\"]], on=\"SecuritiesCode\", how=\"left\")\n",
    "        \n",
    "        sector_dict = {}\n",
    "        for i, c in enumerate(sorted(df_feats[\"33SectorCode\"].unique())):\n",
    "            sector_dict[c] = i\n",
    "\n",
    "        df_feats[\"33SectorCode\"] = df_feats[\"33SectorCode\"].map(sector_dict)\n",
    "        \n",
    "        # weekday\n",
    "        df_feats[\"weekday\"] = df_feats[\"Date\"].dt.weekday\n",
    "        \n",
    "        df_feats = df_feats.replace([-np.inf, np.inf], np.nan)\n",
    "        \n",
    "        return df_feats\n",
    "    \n",
    "    def reduce_data(self, date):\n",
    "        self.markets      = self.markets[self.markets[\"Date\"]>=date]\n",
    "        self.stock_prices = self.stock_prices[self.stock_prices[\"Date\"]>=date]\n",
    "        self.options      = self.options[self.options[\"Date\"]>=date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "isTRAIN = False\n",
    "USE_SUPPLEMENT = True\n",
    "\n",
    "jpx = JPX(isTRAIN, USE_SUPPLEMENT)\n",
    "jpx.initialize()\n",
    "jpx.create_fundamentals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5509b",
   "metadata": {},
   "source": [
    "# Feature Engineering of Stock-based Feature and Time-based Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd7646",
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_PERIOD = \"2020-12-31\"\n",
    "umap_df = pd.read_csv(f\"input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\", usecols=[\"Date\", \"SecuritiesCode\", \"Close\"])\n",
    "umap_df[\"Date\"] = pd.to_datetime(umap_df[\"Date\"])\n",
    "umap_df = umap_df[umap_df[\"Date\"]<=UMAP_PERIOD]\n",
    "\n",
    "umap_df = umap_df.pivot(index=\"Date\", columns=\"SecuritiesCode\", values=\"Close\").copy()\n",
    "umap_df = umap_df.pct_change(1).fillna(0)\n",
    "\n",
    "umap_dim = 100\n",
    "n_neighbors = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d816c11a",
   "metadata": {},
   "source": [
    "## Stock UMAP Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isTRAIN:\n",
    "    # stock UMAP model based on daily return\n",
    "    stock_umap_model_rt = umap.UMAP(n_components=umap_dim, n_neighbors=n_neighbors, random_state=0, n_jobs=-1)\n",
    "\n",
    "    stock_umap_res_rt = pd.DataFrame(\n",
    "        stock_umap_model_rt.fit_transform(umap_df.T),\n",
    "        index=umap_df.columns,\n",
    "        columns=[\"StockUmapRt\"+str(i) for i in range(umap_dim)]\n",
    "    )\n",
    "\n",
    "    # stock UMAP model based on return correlation\n",
    "    stock_umap_model_corr = umap.UMAP(n_components=umap_dim, n_neighbors=n_neighbors, random_state=0, n_jobs=-1)\n",
    "\n",
    "    stock_umap_res_corr = pd.DataFrame(\n",
    "        stock_umap_model_corr.fit_transform(umap_df.corr().fillna(0)),\n",
    "        index=umap_df.columns,\n",
    "        columns=[\"StockUmapCorr\"+str(i) for i in range(umap_dim)]\n",
    "    )\n",
    "else:\n",
    "    # stock UMAP model based on daily return\n",
    "    stock_umap_model_rt = pickle_load(\"input/jpx-pre-trained-model/stock_umap_model_rt.pickle\")\n",
    "\n",
    "    stock_umap_res_rt = pd.DataFrame(\n",
    "        stock_umap_model_rt.transform(umap_df.T),\n",
    "        index=umap_df.columns,\n",
    "        columns=[\"StockUmapRt\"+str(i) for i in range(umap_dim)]\n",
    "    )\n",
    "    \n",
    "    # stock UMAP model based on return correlation\n",
    "    stock_umap_model_corr = pickle_load(\"input/jpx-pre-trained-model/stock_umap_model_corr.pickle\")\n",
    "\n",
    "    stock_umap_res_corr = pd.DataFrame(\n",
    "        stock_umap_model_corr.transform(umap_df.corr().fillna(0)),\n",
    "        index=umap_df.columns,\n",
    "        columns=[\"StockUmapCorr\"+str(i) for i in range(umap_dim)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4015e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_umap_res_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393de42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_umap_res_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194df76",
   "metadata": {},
   "source": [
    "## Time UMAP Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isTRAIN:\n",
    "    # time UMAP model based on daily return\n",
    "    time_umap_model_rt = umap.UMAP(n_components=umap_dim, n_neighbors=n_neighbors, random_state=0)\n",
    "\n",
    "    time_umap_res_rt = pd.DataFrame(\n",
    "        time_umap_model_rt.fit_transform(umap_df),\n",
    "        index=umap_df.index,\n",
    "        columns=[\"TimeUmapRt\"+str(i) for i in range(umap_dim)]\n",
    "    )\n",
    "else:\n",
    "    # time UMAP model based on daily return\n",
    "    time_umap_model_rt = pickle_load(\"input/jpx-pre-trained-model/time_umap_model_rt.pickle\")\n",
    "\n",
    "    time_umap_res_rt = pd.DataFrame(\n",
    "        time_umap_model_rt.transform(umap_df),\n",
    "        index=umap_df.index,\n",
    "        columns=[\"TimeUmapRt\"+str(i) for i in range(umap_dim)]\n",
    "    )\n",
    "\n",
    "def get_time_umap_feats(df, dates=[]):\n",
    "    tmp = df.pivot(index=\"Date\", columns=\"SecuritiesCode\", values=\"Close\").copy()\n",
    "    tmp = tmp.pct_change(1).fillna(0)\n",
    "    if len(dates) != 0:\n",
    "        tmp = tmp[tmp.index.isin(dates)]\n",
    "    \n",
    "    tmp_res = pd.DataFrame(\n",
    "        time_umap_model_rt.transform(tmp),\n",
    "        index=tmp.index,\n",
    "        columns=[\"TimeUmapRt\"+str(i) for i in range(umap_dim)]\n",
    "    )\n",
    "    \n",
    "    tmp_res.reset_index(inplace=True)\n",
    "    \n",
    "    return tmp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64680f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_umap_res_rt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1e8e4",
   "metadata": {},
   "source": [
    "## Option Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a857e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contracts(x):\n",
    "    if   x < \"2017-03-01\": return [201703, 201706, 201709]\n",
    "    elif x < \"2017-06-01\": return [201706, 201709, 201712]\n",
    "    elif x < \"2017-09-01\": return [201709, 201712, 201803]\n",
    "    elif x < \"2017-12-01\": return [201712, 201803, 201806]\n",
    "    elif x < \"2018-03-01\": return [201803, 201806, 201809]\n",
    "    elif x < \"2018-06-01\": return [201806, 201809, 201812]\n",
    "    elif x < \"2018-09-01\": return [201809, 201812, 201903]\n",
    "    elif x < \"2018-12-01\": return [201812, 201903, 201906]\n",
    "    elif x < \"2019-03-01\": return [201903, 201906, 201909]\n",
    "    elif x < \"2019-06-01\": return [201906, 201909, 201912]\n",
    "    elif x < \"2019-09-01\": return [201909, 201912, 202003]\n",
    "    elif x < \"2019-12-01\": return [201912, 202003, 202006]\n",
    "    elif x < \"2020-03-01\": return [202003, 202006, 202009]\n",
    "    elif x < \"2020-06-01\": return [202006, 202009, 202012]\n",
    "    elif x < \"2020-09-01\": return [202009, 202012, 202103]\n",
    "    elif x < \"2020-12-01\": return [202012, 202103, 202106]\n",
    "    elif x < \"2021-03-01\": return [202103, 202106, 202109]\n",
    "    elif x < \"2021-06-01\": return [202106, 202109, 202112]\n",
    "    elif x < \"2021-09-01\": return [202109, 202112, 202203]\n",
    "    elif x < \"2021-12-01\": return [202112, 202203, 202206]\n",
    "    elif x < \"2022-03-01\": return [202203, 202206, 202209]\n",
    "    elif x < \"2022-06-01\": return [202206, 202209, 202212]\n",
    "    elif x < \"2022-09-01\": return [202209, 202212, 202303]\n",
    "    else                 : return [202212, 202303, 202306]\n",
    "\n",
    "def get_option_feats(tmp_markets, tmp_options):\n",
    "    tmp_markets[\"DateStr\"]  = tmp_markets[\"Date\"].astype(\"str\")\n",
    "    tmp_markets[\"contract\"] = tmp_markets[\"DateStr\"].apply(get_contracts)\n",
    "    \n",
    "    civs = []   # Implied Volatility of call options\n",
    "    pivs = []   # Implied Volatility of put options\n",
    "    \n",
    "    for dt in tmp_markets[\"Date\"].unique():\n",
    "        try:\n",
    "            tmp = tmp_options[tmp_options[\"Date\"]==dt].copy()\n",
    "            tmp = tmp[tmp[\"ContractMonth\"].isin(tmp_markets.loc[tmp_markets[\"Date\"]==dt, \"contract\"].values[0])]\n",
    "            \n",
    "            strikeprices = tmp[\"StrikePrice\"].unique()\n",
    "            atm = strikeprices[np.argmin(abs(strikeprices - tmp_markets.loc[tmp_markets[\"Date\"]==dt, \"NIKKEI\"].values[0]))]\n",
    "            \n",
    "            tmp = tmp[tmp[\"StrikePrice\"]==atm]\n",
    "            civ = tmp.loc[tmp[\"Putcall\"]==2, \"ImpliedVolatility\"].mean()\n",
    "            piv = tmp.loc[tmp[\"Putcall\"]==1, \"ImpliedVolatility\"].mean()\n",
    "            \n",
    "            civs.append(civ)\n",
    "            pivs.append(piv)\n",
    "        except:\n",
    "            civs.append(np.nan)\n",
    "            pivs.append(np.nan)\n",
    "    \n",
    "    tmp_markets[\"civ\"] = civs\n",
    "    tmp_markets[\"piv\"] = pivs\n",
    "    tmp_markets[\"diff_civ\"] = tmp_markets[\"civ\"].diff(1)\n",
    "    tmp_markets[\"diff_piv\"] = tmp_markets[\"piv\"].diff(1)\n",
    "    tmp_markets[\"iv_spread\"] = tmp_markets[\"civ\"] - tmp_markets[\"piv\"]\n",
    "    tmp_markets[\"diff_iv_spread\"] = tmp_markets[\"iv_spread\"].diff(1)\n",
    "    \n",
    "    tmp_markets = tmp_markets[[\"Date\", \"iv_spread\", \"diff_civ\", \"diff_piv\", \"diff_iv_spread\"]].fillna(0)\n",
    "    \n",
    "    return tmp_markets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b4ad7f",
   "metadata": {},
   "source": [
    "## Time Regime Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cceff80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regime_feats(df):\n",
    "    feats = [\"Date\", \"33SectorCode\", \"ror1\", \"ror22\", \"ror252\", \"trading_value\", \"trading_value5\", \"trading_value20\", \"volatility252\", \"topix_corr\", \"nikkei_corr\", \"Equity\", \"EquityRatio\", \"PBR\"]\n",
    "    tmp_df = df[feats].copy()\n",
    "    \n",
    "    # time aggregate feature\n",
    "    agg_feats = [\"ror1\", \"ror22\", \"ror252\", \"trading_value\", \"trading_value5\", \"trading_value20\", \"volatility252\", \"topix_corr\", \"nikkei_corr\"]\n",
    "    \n",
    "    tmp = tmp_df.groupby(\"Date\")[agg_feats].mean().add_suffix(\"_timemean_regime\").reset_index()\n",
    "    df = df.merge(tmp, on=\"Date\", how=\"left\")\n",
    "\n",
    "    tmp = tmp_df.groupby(\"Date\")[agg_feats].std().add_suffix(\"_timestd_regime\").reset_index()\n",
    "    df = df.merge(tmp, on=\"Date\", how=\"left\")\n",
    "    \n",
    "    # sector regime feature\n",
    "    for f in [\"ror1\", \"ror22\"]:\n",
    "        tmp = tmp_df.groupby([\"Date\", \"33SectorCode\"])[f].mean().reset_index().pivot(index=\"Date\", columns=\"33SectorCode\", values=f).add_suffix(\"_sector_regime_\"+f).reset_index()\n",
    "        df = df.merge(tmp, on=\"Date\", how=\"left\")\n",
    "    \n",
    "    # factor regime feature\n",
    "    fr_feats = [\"Equity\", \"EquityRatio\", \"PBR\", \"ror252\", \"topix_corr\", \"volatility252\"]\n",
    "\n",
    "    tmp1  = []\n",
    "    tmp22 = []\n",
    "    \n",
    "    for f in fr_feats:\n",
    "        tmp1.append(tmp_df.groupby(\"Date\")[f].corr(tmp_df[\"ror1\"]))\n",
    "        tmp22.append(tmp_df.groupby(\"Date\")[f].corr(tmp_df[\"ror22\"]))\n",
    "\n",
    "    tmp = pd.concat([pd.concat(tmp1, axis=1).add_suffix(\"_factor_regime1\"), pd.concat(tmp22, axis=1).add_suffix(\"_factor_regime22\")], axis=1).reset_index()\n",
    "    df = df.merge(tmp, on=\"Date\", how=\"left\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af9b3b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c96b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(train_df, x_feats, cat_feats, target, seed=0):\n",
    "    model = LGBMRegressor(max_depth=4, learning_rate=0.01, num_leaves=20, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1, random_state=seed)\n",
    "    model.fit(train_df[x_feats], train_df[target], categorical_feature=cat_feats, verbose=False)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_feature_list(df):\n",
    "    x_feats    = [f for f in df.columns if f not in [\"RowId\", \"Date\", \"SecuritiesCode\", \"Target\"]]\n",
    "    x_feats    = [f for f in x_feats if f not in ['YearlyNetSales', 'YearlyOperatingProfit', 'YearlyOrdinaryProfit', 'YearlyProfit', 'ForecastNetSales', 'ForecastOperatingProfit',\n",
    "                                                  'ForecastOrdinaryProfit', 'ForecastProfit', 'Expense1', 'Expense2', 'Expense3', 'ForecastExpense1', 'ForecastExpense2',\n",
    "                                                  'ForecastExpense3']]\n",
    "    cat_feats       = [\"33SectorCode\", \"stop_close\", \"stop_open\", \"uptick\", \"weekday\"]\n",
    "    regime_feats    = [f for f in x_feats if \"regime\" in f]\n",
    "    option_feats    = [\"iv_spread\", \"diff_civ\", \"diff_piv\", \"diff_iv_spread\"]\n",
    "    time_umap_feats = [f for f in x_feats if \"TimeUmap\" in f]\n",
    "    neut_feats      = [f for f in x_feats if f not in cat_feats+regime_feats+option_feats+time_umap_feats]\n",
    "    \n",
    "    return x_feats, cat_feats, neut_feats\n",
    "\n",
    "\n",
    "# dataFrame for training\n",
    "TRAIN_PERIOD = \"2020-12-31\"\n",
    "\n",
    "df_feats = jpx.get_feature()\n",
    "df_feats = get_regime_feats(df_feats)\n",
    "df_feats = df_feats.merge(get_option_feats(jpx.markets.copy(), jpx.options.copy()), on=\"Date\", how=\"left\")\n",
    "df_feats = df_feats.merge(get_time_umap_feats(jpx.stock_prices), on=\"Date\", how=\"left\")\n",
    "\n",
    "df_train = df_feats[(df_feats[\"Date\"]<=TRAIN_PERIOD) & (df_feats[\"Date\"].dt.year!=2020)].copy()\n",
    "df_valid = df_feats[df_feats[\"Date\"]> TRAIN_PERIOD].copy()\n",
    "\n",
    "# feature lists\n",
    "x_feats, cat_feats, neut_feats = get_feature_list(df_feats)\n",
    "target = [\"Target\"]\n",
    "\n",
    "if isTRAIN:\n",
    "    # trained in GCP, region:us-west1-b, machine type:N1 high memory (16vCPUs, 104GB RAM)\n",
    "    model1 = train_lgbm(df_train, x_feats, cat_feats, target, seed=1)\n",
    "    model2 = train_lgbm(df_train, x_feats, cat_feats, target, seed=24)\n",
    "    model3 = train_lgbm(df_train, x_feats, cat_feats, target, seed=39)\n",
    "    model4 = train_lgbm(df_train, x_feats, cat_feats, target, seed=114)\n",
    "    model5 = train_lgbm(df_train, x_feats, cat_feats, target, seed=251)\n",
    "else:\n",
    "    model1 = pickle_load(\"input/jpx-pre-trained-model/jpx_lgbm_01.pickle\")\n",
    "    model2 = pickle_load(\"input/jpx-pre-trained-model/jpx_lgbm_02.pickle\")\n",
    "    model3 = pickle_load(\"input/jpx-pre-trained-model/jpx_lgbm_03.pickle\")\n",
    "    model4 = pickle_load(\"input/jpx-pre-trained-model/jpx_lgbm_04.pickle\")\n",
    "    model5 = pickle_load(\"input/jpx-pre-trained-model/jpx_lgbm_05.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daee2af",
   "metadata": {},
   "source": [
    "# Neutralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74458d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize_pred(df, neut_feats):\n",
    "    df = df.replace([-np.inf, np.inf], np.nan).fillna(0)\n",
    "    df = pd.get_dummies(df, columns=[\"33SectorCode\"])\n",
    "    df = df.merge(stock_umap_res_rt.reset_index(),   on=\"SecuritiesCode\", how=\"left\")\n",
    "    df = df.merge(stock_umap_res_corr.reset_index(), on=\"SecuritiesCode\", how=\"left\")\n",
    "    \n",
    "    sector_feats = [f for f in df.columns if \"Sector\" in f]\n",
    "    sector_feats = sector_feats[:-1]\n",
    "    \n",
    "    umap_feats_rt   = [f for f in df.columns if \"UmapRt\" in f]\n",
    "    umap_feats_corr = [f for f in df.columns if \"UmapCorr\" in f]\n",
    "    \n",
    "    neut_feats = neut_feats + sector_feats + umap_feats_rt + umap_feats_corr\n",
    "    \n",
    "    df[\"pred_nt\"] = np.nan\n",
    "    \n",
    "    for dt in sorted(df[\"Date\"].unique()):\n",
    "        tmp_df = df[df[\"Date\"]==dt].copy()\n",
    "        lm = LinearRegression()\n",
    "        lm.fit(tmp_df[neut_feats], tmp_df[\"pred\"])\n",
    "        tmp_df[\"pred_nt\"] = tmp_df[\"pred\"] - lm.predict(tmp_df[neut_feats])\n",
    "        \n",
    "        df.loc[df[\"Date\"]==dt, \"pred_nt\"] = tmp_df[\"pred_nt\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de895113",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a76b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spread_return_per_day(df, portfolio_size=200, toprank_weight_ratio=2.0, obj=\"Rank_nt\"):\n",
    "    weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)\n",
    "    purchase = (df.sort_values(by=obj)[\"Target\"][:portfolio_size] * weights).sum() / weights.mean()\n",
    "    short = (df.sort_values(by=obj, ascending=False)[\"Target\"][:portfolio_size] * weights).sum() / weights.mean()\n",
    "    return purchase - short\n",
    "\n",
    "def calc_spread_return_sharpe(df, portfolio_size=200, toprank_weight_ratio=2.0, obj=\"Rank_nt\"):\n",
    "    buf = df.groupby(\"Date\").apply(calc_spread_return_per_day, portfolio_size, toprank_weight_ratio, obj)\n",
    "    sharpe_ratio = buf.mean() / buf.std()\n",
    "    return sharpe_ratio, buf.mean(), buf.std(), buf\n",
    "\n",
    "def plot_results(df):\n",
    "    df[\"Rank\"]    = df.groupby(\"Date\")[\"pred\"].rank(ascending=False)\n",
    "    df[\"Rank_nt\"] = df.groupby(\"Date\")[\"pred_nt\"].rank(ascending=False)\n",
    "    \n",
    "    sr, mean, std, res = calc_spread_return_sharpe(df, 200, 2.0, \"Rank\")\n",
    "    sr_nt, mean_nt, std_nt, res_nt = calc_spread_return_sharpe(df, 200, 2.0, \"Rank_nt\")\n",
    "    sr1, mean1, std1, _ = calc_spread_return_sharpe(df[(df[\"Date\"]>=\"2021-01-01\") & (df[\"Date\"]<\"2021-04-01\")], 200, 2.0, \"Rank_nt\")\n",
    "    sr2, mean2, std2, _ = calc_spread_return_sharpe(df[(df[\"Date\"]>=\"2021-04-01\") & (df[\"Date\"]<\"2021-07-01\")], 200, 2.0, \"Rank_nt\")\n",
    "    sr3, mean3, std3, _ = calc_spread_return_sharpe(df[(df[\"Date\"]>=\"2021-07-01\") & (df[\"Date\"]<\"2021-10-01\")], 200, 2.0, \"Rank_nt\")\n",
    "    sr4, mean4, std4, _ = calc_spread_return_sharpe(df[(df[\"Date\"]>=\"2021-10-01\") & (df[\"Date\"]<\"2022-01-01\")], 200, 2.0, \"Rank_nt\")\n",
    "    sr5, mean5, std5, _ = calc_spread_return_sharpe(df[(df[\"Date\"]>=\"2022-01-01\") & (df[\"Date\"]<\"2022-04-01\")], 200, 2.0, \"Rank_nt\")\n",
    "    sr6, mean6, std6, _ = calc_spread_return_sharpe(df[(df[\"Date\"]>=\"2022-04-01\") & (df[\"Date\"]<\"2022-07-01\")], 200, 2.0, \"Rank_nt\")\n",
    "    sr7, mean7, std7, _ = calc_spread_return_sharpe(df[(df[\"Date\"]>=\"2021-12-06\") & (df[\"Date\"]<\"2022-03-01\")], 200, 2.0, \"Rank_nt\")\n",
    "    \n",
    "    print(f\"validation total sharpe : {sr_nt}\")\n",
    "    print(f\"public LB sharpe : {sr7}\")\n",
    "    \n",
    "    res = pd.DataFrame(res)\n",
    "    res.columns = [\"pnl\"]\n",
    "    res_nt = pd.DataFrame(res_nt)\n",
    "    res_nt.columns = [\"pnl_nt\"]\n",
    "    res = res.join(res_nt)\n",
    "    res = res.join(jpx.markets[[\"Date\", \"NIKKEI\"]].set_index(\"Date\"))\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    ax2 = fig.add_subplot(234)\n",
    "    ax3 = fig.add_subplot(235)\n",
    "    ax4 = fig.add_subplot(236)\n",
    "    \n",
    "    # plot balance curve\n",
    "    ax1.plot(res[\"NIKKEI\"])\n",
    "    ax1.legend([\"NIKKEI\"], loc='upper left')\n",
    "    \n",
    "    ax5 = ax1.twinx()\n",
    "    ax5.plot(res[\"pnl\"].cumsum(), color=\"red\")\n",
    "    ax5.plot(res[\"pnl_nt\"].cumsum(), color=\"green\")\n",
    "    ax5.legend([\"pnl without neutralization\", \"pnl with neutralization\"], loc='lower right')\n",
    "    \n",
    "    # plot sharpe ratio\n",
    "    ax2.bar([\"21-1Q\", \"21-2Q\", \"21-3Q\", \"21-4Q\", \"22-1Q\", \"22-2Q\"], [sr1, sr2, sr3, sr4, sr5, sr6])\n",
    "    ax2.set_title(\"quarterly sharpe ratio\")\n",
    "    \n",
    "    # plot mean\n",
    "    ax3.bar([\"21-1Q\", \"21-2Q\", \"21-3Q\", \"21-4Q\", \"22-1Q\", \"22-2Q\"], [mean1, mean2, mean3, mean4, mean5, mean6])\n",
    "    ax3.set_title(\"quarterly pnl mean\")\n",
    "    \n",
    "    # plot std\n",
    "    ax4.bar([\"21-1Q\", \"21-2Q\", \"21-3Q\", \"21-4Q\", \"22-1Q\", \"22-2Q\"], [std1, std2, std3, std4, std5, std6])\n",
    "    ax4.set_title(\"quarterly pnl std\")\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid[\"pred1\"] = model1.predict(df_valid[x_feats])\n",
    "df_valid[\"pred2\"] = model2.predict(df_valid[x_feats])\n",
    "df_valid[\"pred3\"] = model3.predict(df_valid[x_feats])\n",
    "df_valid[\"pred4\"] = model4.predict(df_valid[x_feats])\n",
    "df_valid[\"pred5\"] = model5.predict(df_valid[x_feats])\n",
    "\n",
    "df_valid[\"pred\"] = df_valid[[\"pred1\", \"pred2\", \"pred3\", \"pred4\", \"pred5\"]].mean(axis=1)\n",
    "df_valid = neutralize_pred(df_valid, neut_feats)\n",
    "df_valid[\"pred_nt\"] = df_valid[\"pred_nt\"].replace([-np.inf, np.inf], np.nan).fillna(0)\n",
    "\n",
    "plot_results(df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7ac8d",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89216cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpx.reduce_data(date=\"2020-11-15\")\n",
    "del df_feats, df_train, df_valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049225e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import jpx_tokyo_market_prediction\n",
    "\n",
    "env = jpx_tokyo_market_prediction.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "for (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n",
    "    # update data\n",
    "    jpx.update_prices(prices)\n",
    "    jpx.update_markets(secondary_prices)\n",
    "    jpx.update_options(options)\n",
    "    \n",
    "    if len(financials) != 0:\n",
    "        update_fins_codes = [c for c in financials[\"SecuritiesCode\"].unique() if c in jpx.univ_codes]\n",
    "        if len(update_fins_codes) != 0:\n",
    "            jpx.update_fins(financials)\n",
    "            jpx.update_fundamentals(update_fins_codes)\n",
    "    \n",
    "    # make feature\n",
    "    df_feats = jpx.get_feature()\n",
    "    df_feats = df_feats[df_feats[\"Date\"]==sample_prediction[\"Date\"].iloc[0]]\n",
    "    df_feats = get_regime_feats(df_feats)\n",
    "    df_feats = df_feats.merge(get_option_feats(jpx.markets.copy(), jpx.options.copy()), on=\"Date\", how=\"left\")\n",
    "    df_feats = df_feats.merge(get_time_umap_feats(jpx.stock_prices, dates=[sample_prediction[\"Date\"].iloc[0]]), on=\"Date\", how=\"left\")\n",
    "    \n",
    "    # feature lists\n",
    "    x_feats, cat_feats, neut_feats = get_feature_list(df_feats)\n",
    "    \n",
    "    # prediction & neutralization\n",
    "    df_feats[\"pred1\"] = model1.predict(df_feats[x_feats])\n",
    "    df_feats[\"pred2\"] = model2.predict(df_feats[x_feats])\n",
    "    df_feats[\"pred3\"] = model3.predict(df_feats[x_feats])\n",
    "    df_feats[\"pred4\"] = model4.predict(df_feats[x_feats])\n",
    "    df_feats[\"pred5\"] = model5.predict(df_feats[x_feats])\n",
    "    \n",
    "    df_feats[\"pred\"] = df_feats[[\"pred1\", \"pred2\", \"pred3\", \"pred4\", \"pred5\"]].mean(axis=1)\n",
    "    \n",
    "    df_feats = neutralize_pred(df_feats, neut_feats)\n",
    "    df_feats[\"pred_nt\"] = df_feats[\"pred_nt\"].replace([-np.inf, np.inf], np.nan).fillna(0)\n",
    "    \n",
    "    # submission\n",
    "    sample_prediction = sample_prediction.merge(df_feats[[\"SecuritiesCode\", \"pred_nt\"]], on=\"SecuritiesCode\", how=\"left\")\n",
    "    sample_prediction.sort_values(\"pred_nt\", ascending=False, inplace=True)\n",
    "    #sample_prediction[\"Rank\"] = np.arange(0,2000)\n",
    "    sample_prediction[\"Rank\"] = np.arange(0,len(sample_prediction))\n",
    "    sample_prediction.sort_values(\"SecuritiesCode\", ascending=True, inplace=True)\n",
    "    submission = sample_prediction[[\"Date\",\"SecuritiesCode\",\"Rank\"]]\n",
    "    env.predict(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f4811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
